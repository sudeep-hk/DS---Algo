{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openapi(summary,skills):\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"davinci\",\n",
    "      prompt=\"\"\"\n",
    "        This is a JT and JD extractor.\n",
    "        Summary: Having 3.6 Years experience (3 years in Pyspark and Spark using Scala) on Projects in Judicial, Retail,\n",
    "                Pharma Services industries with quick learning ability , performing ETL Operations with primary focus in\n",
    "                Developing Spark Scripts, Optimization,Data Cleansing , Data Profiling ,Data Wrangling , Data Ingestion\n",
    "                and Data Loading. Experience of BIG DATA DEVELOPER with around 3 years on project in Judicial services,\n",
    "                Retail, Pharma industries performing ETL operations and primary focus on Data Cleansing,\n",
    "                Data Profiling, Data ingestion and Data Loading.\n",
    "                Experience in all phases of Software Development Life Cycle. Direct interaction with\n",
    "                business community, collecting functional requirements , Analysis , Design , Development ,\n",
    "                Implementation , Enhancement , Maintenance and Production support .\n",
    "                Experience in ETL projects and ETL tools technologies like Spark, HIVE and Hadoop .\n",
    "                Developing Spark applications using multiple programming language Python, Scala.\n",
    "                Experience in processing structured and semi-structured in Hadoop using Spark\n",
    "                Good knowledge in HIVE Warehouse and Internal/External tables, Partitioning, Bucketing,\n",
    "                Joins Optimization of HIVE Warehouse\n",
    "                Experience in writing SQL Queries.\n",
    "                Good understanding of Data Warehousing concepts (SCD's, Dimension & Fact Tables).\n",
    "                Direct Interaction with business community, collecting/creating functional requirements\n",
    "                and converting them to technical specifications.\n",
    "                Good multi-tasking skills with flexible timings and good communication skills.\n",
    "        Skills: Scala,Python, SQL,Big Data Ecosystem Apache Spark, Hive, HDFS, Hadoop, YARN,Google\n",
    "        Cloud Platform (GCP),Apache Airflow,Data Normalization and Warehousing, MySQL\n",
    "        Job Description:A data engineer, As part of our team, your main responsibilities will focus in area of:\n",
    "                Development of data engineering solutions for new and ongoing projects in DW and Big Data area,\n",
    "                Large-scale deployments, automation and configuration management.\n",
    "                Technology explorations, research & development, deep investigations & troubleshooting\n",
    "###\n",
    "        Summary: {summary}\n",
    "        Skills: {skills}\n",
    "        Job Description:\"\"\".format(summary=summary,skills=skills),\n",
    "      temperature=0.3,\n",
    "      max_tokens=60,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "      stop=[\"###\"]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ans(response):\n",
    "    return print('Job desc: ', response.choices[0].text.partition('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=\"\"\"develops and maintains highly scalable, secure and reliable data\n",
    "structures. Accustomed to working closely with system architects,data scientists and design analysts to understand\n",
    "business or industry requirements to develop comprehensive data models. Proficient at developing ETL pipelines at the\n",
    "modeling, design and implementation stages.Created python application to automate the process of manual rotation of passwords for the generic accounts\n",
    "used by different pipelines in PAM Portal, which is an UI to access CyberArk's CCP Safes.\n",
    "Created Venafi Certificates to connect different generic accounts with Non-Prod and Prod CCP safes of CyberArk\n",
    "and Applied proper authenticating methods to provide security to the process using key tab.\n",
    "Created the Bamboo plan and Airflow Dags to deploy the source code in airflow cluster.\n",
    "Worked on CI/CD and airflow for continuous development and deployment using stash/bitbucket and bamboo.\n",
    "Tools : Python, Bitbucket, Airflow, Bamboo.\"\"\"\n",
    "skills=\"\"\"Spark SQL, Spark Core, Python, Java, Scala, SQL, Drools, Azure Databricks, Notebooks, Bamboo & Arturo (CI/CD),\n",
    "Git/Bitbucket, Hadoop, Hive & HBase & Postgres , Hue & Impala, Oozie & Airflow Scheduler, Control-M, Kafka & Flume,\n",
    "Bash/python Shell Scripting\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=call_openapi(summary,skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6n7QJ64OmYVt5n7hiOWrQurUqiS02 at 0x1758ebe1760> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" \\u00a0Developed a python application to automate the process of manual rotation of passwords for the generic accounts used by different pipelines in PAM Portal, which is an UI to access CyberArk's CCP Safes.\\nCreated Venafi Certificates to connect different generic accounts with Non-Prod and\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1677164627,\n",
       "  \"id\": \"cmpl-6n7QJ64OmYVt5n7hiOWrQurUqiS02\",\n",
       "  \"model\": \"davinci\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 60,\n",
       "    \"prompt_tokens\": 1100,\n",
       "    \"total_tokens\": 1160\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb27270a5e4f8cb7d0eca249a0950dc7daa5204e8bcb6dd237418e87fe5c3217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
